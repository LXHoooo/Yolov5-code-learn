# YOLOv5 ğŸš€ by Ultralytics, GPL-3.0 license
"""
PyTorch utils
"""

import math
import os
import platform   # æä¾›è·å–æ“ä½œç³»ç»Ÿç›¸å…³ä¿¡æ¯çš„æ¨¡å—
import subprocess  # å­è¿›ç¨‹å®šä¹‰åŠæ“ä½œçš„æ¨¡å—
import time
import warnings
from contextlib import contextmanager  # ç”¨äºè¿›è¡Œä¸Šä¸‹æ–‡ç®¡ç†çš„æ¨¡å—
from copy import deepcopy
from pathlib import Path  # pathå°†strè½¬æ¢ä¸ºpathå¯¹è±¡ï¼Œä½¿å­—ç¬¦è·¯å¾„æ˜“äºæ“ä½œçš„æ¨¡å—

import torch
import torch.distributed as dist
import torch.nn as nn
import torch.nn.functional as F

from utils.general import LOGGER, file_update_date, git_describe

try:
    import thop  # for FLOPs computation ç”¨äºpytorchæ¨¡å‹çš„FLOPSè®¡ç®—å·¥å…·æ¨¡å—
except ImportError:
    thop = None

# Suppress PyTorch warnings
warnings.filterwarnings('ignore', message='User provided device_type of \'cuda\', but CUDA is not available. Disabling')


# è¿™ä¸ªå‡½æ•°æ˜¯ç”¨æ¥å¤„ç†æ¨¡å‹è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒæ—¶çš„åŒæ­¥é—®é¢˜
@contextmanager
def torch_distributed_zero_first(local_rank: int):
    """
    ç”¨äºtrain.pyä¸­
    ç”¨äºå¤„ç†æ¨¡å‹è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒæ—¶åŒæ­¥é—®é¢˜
    åŸºäºtorch.distributed.barrier()å‡½æ•°çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œä¸ºäº†å®Œæˆæ•°æ®çš„æ­£å¸¸åŒæ­¥æ“ä½œ
    :param local_rank:ä»£è¡¨å½“å‰è¿›ç¨‹å·ï¼Œ0ä»£è¡¨ä¸»è¿›ç¨‹ï¼Œ1ã€2ã€3ä»£è¡¨å­è¿›ç¨‹
    :return:
    """
    # Decorator to make all processes in distributed training wait for each local_master to do something
    if local_rank not in [-1, 0]:
        # å¦‚æœæ‰§è¡Œcreate_dataloader()å‡½æ•°çš„è¿›ç¨‹ä¸hiä¸»è¿›ç¨‹ï¼Œå³rankä¸ç­‰äº0æˆ–è€…-1
        # ä¸Šä¸‹æ–‡ç®¡ç†å™¨ä¼šæ‰§è¡Œç›¸åº”çš„torch.distributed.barrier()ï¼Œè¿™æ˜¯ä¸€ä¸ªé˜»å¡æ ï¼Œ
        # è®©æ­¤è¿›ç¨‹å¤„äºç­‰å¾…çŠ¶æ€ï¼Œç­‰å¾…æ‰€æœ‰è¿›ç¨‹åˆ°è¾¾æ …æ å¤„ï¼ˆåŒ…æ‹¬ä¸»è¿›ç¨‹æ•°æ®å¤„ç†å®Œæ¯•ï¼‰
        dist.barrier(device_ids=[local_rank])
    yield  # yieldè¯­å¥ä¸­ï¼Œä¸­æ–­åæ‰§è¡Œä¸Šä¸‹æ–‡ä»£ç ï¼Œç„¶åè¿”å›åˆ°æ­¤å¤„ç»§ç»­å¾€ä¸‹æ‰§è¡Œ
    if local_rank == 0:
        # å¦‚æœæ‰§è¡Œcreate_dataloader()å‡½æ•°çš„è¿›ç¨‹æ—¶ä¸»è¿›ç¨‹ï¼Œå…¶ä¼šç›´æ¥å»è¯»å–æ•°æ®å¹¶å¤„ç†ï¼Œ
        # ç„¶åå…¶å¤„ç†ç»“æŸä¹‹åä¼šæ¥ç€é‡åˆ°torch.distributed.barrier(),
        # æ­¤æ—¶ï¼Œæ‰€æœ‰è¿›ç¨‹éƒ½åˆ°è¾¾äº†å½“å‰çš„æ …æ å¤„ï¼Œè¿™æ ·æ‰€æœ‰è¿›ç¨‹å°±è¾¾åˆ°äº†åŒæ­¥ï¼Œå¹¶åŒæ—¶å¾—åˆ°é‡Šæ”¾
        dist.barrier(device_ids=[0])


def device_count():
    # Returns number of CUDA devices available. Safe version of torch.cuda.device_count(). Only works on Linux.
    assert platform.system() == 'Linux', 'device_count() function only works on Linux'
    try:
        cmd = 'nvidia-smi -L | wc -l'
        return int(subprocess.run(cmd, shell=True, capture_output=True, check=True).stdout.decode().split()[-1])
    except Exception:
        return 0


# è‡ªåŠ¨é€‰æ‹©ç³»ç»Ÿè®¾å¤‡çš„æ“ä½œ
def select_device(device='', batch_size=0, newline=True):
    # device = 'cpu' or '0' or '0,1,2,3'
    """
    å¹¿æ³›ç”¨äºtrain.pyã€test.pyã€detect.pyç­‰æ–‡ä»¶ä¸­
    ç”¨äºé€‰æ‹©æ¨¡å‹è®­ç»ƒçš„è®¾å¤‡ï¼Œå¹¶è¾“å‡ºæ—¥å¿—ä¿¡æ¯
    :param device: è¾“å…¥çš„è®¾å¤‡ï¼Œdevice = 'cpu' or '0' or '0,1,2,3'
    :param batch_size: ä¸€ä¸ªæ‰¹æ¬¡çš„å›¾ç‰‡ä¸ªæ•°
    :param newline:
    :return:
    """
    s = f'YOLOv5 ğŸš€ {git_describe() or file_update_date()} torch {torch.__version__} '  # string
    device = str(device).strip().lower().replace('cuda:', '')  # to string, 'cuda:0' to '0'
    cpu = device == 'cpu'
    if cpu:
        # å¦‚æœcpu=Trueï¼Œå°±å¼ºåˆ¶ä½¿ç”¨cpuï¼Œä»¤torch.cuda.is_available() = False
        os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # force torch.cuda.is_available() = False
    elif device:  # non-cpu device requested
        # å¦‚æœè¾“å…¥deviceä¸ä¸ºç©ºï¼Œdevice=GPU,ç›´æ¥è®¾ç½®cudaç¯å¢ƒ
        os.environ['CUDA_VISIBLE_DEVICES'] = device  # set environment variable - must be before assert is_available()
        # æ£€æŸ¥cudaçš„å¯ç”¨æ€§ï¼Œå¦‚æœä¸å¯ç”¨åˆ™ç»ˆæ­¢ç¨‹åº
        assert torch.cuda.is_available() and torch.cuda.device_count() >= len(device.replace(',', '')), \
            f"Invalid CUDA '--device {device}' requested, use '--device cpu' or pass valid CUDA device(s)"

    # è¾“å…¥deviceä¸ºç©ºï¼Œè‡ªè¡Œæ ¹æ®è®¡ç®—æœºæƒ…å†µé€‰æ‹©ç›¸åº”è®¾å¤‡ï¼Œå…ˆçœ‹GPUï¼Œæ²¡æœ‰å°±cpu
    # å¦‚æœcudaå¯ç”¨ä¸”è¾“å…¥device != cpuï¼Œåˆ™cuda=True,åæ­£GPUï¼Œæ²¡æœ‰å°±cpu
    cuda = not cpu and torch.cuda.is_available()
    if cuda:
        devices = device.split(',') if device else '0'  # range(torch.cuda.device_count())  # i.e. 0,1,6,7
        n = len(devices)  # device countï¼Œæ‰€æœ‰å¯ç”¨çš„GPUè®¾å¤‡æ•°é‡
        # æ£€æŸ¥æ˜¯å¦æœ‰GPUè®¾å¤‡ï¼Œä¸”batch_sizeæ˜¯å¦å¯ä»¥èƒ½è¢«æ˜¾å¡æ•°ç›®æ•´é™¤
        if n > 1 and batch_size > 0:  # check batch_size is divisible by device_count
            # å¦‚æœä¸èƒ½åˆ™å…³é—­ç¨‹åº
            assert batch_size % n == 0, f'batch-size {batch_size} not multiple of GPU count {n}'
        space = ' ' * (len(s) + 1)
        # æ»¡è¶³æ‰€æœ‰æ¡ä»¶ï¼ŒsåŠ ä¸Šæ‰€æœ‰æ˜¾å¡çš„ä¿¡æ¯
        for i, d in enumerate(devices):
            # p:æ¯ä¸ªå¯ç”¨æ˜¾å¡çš„ç›¸å…³å±æ€§
            p = torch.cuda.get_device_properties(i)
            # æ˜¾ç¤ºä¿¡æ¯såŠ ä¸Šæ¯å¼ æ˜¾å¡çš„å±æ€§ä¿¡æ¯
            s += f"{'' if i == 0 else space}CUDA:{d} ({p.name}, {p.total_memory / (1 << 20):.0f}MiB)\n"  # bytes to MB
    else:
        # cudaä¸å¯ç”¨ï¼Œæ˜¾ç¤ºä¿¡æ¯å°±åŠ ä¸Šcpu
        s += 'CPU\n'

    if not newline:
        s = s.rstrip()
    # å°†æ˜¾ç¤ºä¿¡æ¯såŠ å…¥loggeræ—¥å¿—æ–‡ä»¶ä¸­
    LOGGER.info(s.encode().decode('ascii', 'ignore') if platform.system() == 'Windows' else s)  # emoji-safe
    # å¦‚æœcudaå¯ç”¨ï¼Œå°±è¿”å›ç¬¬ä¸€å¼ æ˜¾å¡çš„åç§°
    return torch.device('cuda:0' if cuda else 'cpu')


# ç”¨äºåœ¨è¿›è¡Œåˆ†å¸ƒå¼æ“ä½œæ—¶ï¼Œç²¾ç¡®çš„è·å–å½“å‰æ—¶é—´
def time_sync():
    # PyTorch-accurate time
    if torch.cuda.is_available():
        torch.cuda.synchronize()
    return time.time()


# æ²¡ç”¨åˆ°ï¼Œç”¨æ¥è¾“å‡ºæŸä¸ªç½‘ç»œç»“æ„çš„ä¸€ä¸‹ä¿¡æ¯
def profile(input, ops, n=10, device=None):
    # YOLOv5 speed/memory/FLOPs profiler
    #
    # Usage:
    #     input = torch.randn(16, 3, 640, 640)
    #     m1 = lambda x: x * torch.sigmoid(x)
    #     m2 = nn.SiLU()
    #     profile(input, [m1, m2], n=100)  # profile over 100 iterations

    results = []
    device = device or select_device()
    print(f"{'Params':>12s}{'GFLOPs':>12s}{'GPU_mem (GB)':>14s}{'forward (ms)':>14s}{'backward (ms)':>14s}"
          f"{'input':>24s}{'output':>24s}")

    for x in input if isinstance(input, list) else [input]:
        x = x.to(device)
        x.requires_grad = True
        for m in ops if isinstance(ops, list) else [ops]:
            m = m.to(device) if hasattr(m, 'to') else m  # device
            m = m.half() if hasattr(m, 'half') and isinstance(x, torch.Tensor) and x.dtype is torch.float16 else m
            tf, tb, t = 0, 0, [0, 0, 0]  # dt forward, backward
            try:
                flops = thop.profile(m, inputs=(x,), verbose=False)[0] / 1E9 * 2  # GFLOPs
            except Exception:
                flops = 0

            try:
                for _ in range(n):
                    t[0] = time_sync()
                    y = m(x)
                    t[1] = time_sync()
                    try:
                        _ = (sum(yi.sum() for yi in y) if isinstance(y, list) else y).sum().backward()
                        t[2] = time_sync()
                    except Exception:  # no backward method
                        # print(e)  # for debug
                        t[2] = float('nan')
                    tf += (t[1] - t[0]) * 1000 / n  # ms per op forward
                    tb += (t[2] - t[1]) * 1000 / n  # ms per op backward
                mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0  # (GB)
                s_in = tuple(x.shape) if isinstance(x, torch.Tensor) else 'list'
                s_out = tuple(y.shape) if isinstance(y, torch.Tensor) else 'list'
                p = sum(list(x.numel() for x in m.parameters())) if isinstance(m, nn.Module) else 0  # parameters
                print(f'{p:12}{flops:12.4g}{mem:>14.3f}{tf:14.4g}{tb:14.4g}{str(s_in):>24s}{str(s_out):>24s}')
                results.append([p, flops, mem, tf, tb, s_in, s_out])
            except Exception as e:
                print(e)
                results.append(None)
            torch.cuda.empty_cache()
    return results


def is_parallel(model):
    # Returns True if model is of type DP or DDP
    return type(model) in (nn.parallel.DataParallel, nn.parallel.DistributedDataParallel)


def de_parallel(model):
    # De-parallelize a model: returns single-GPU model if model is of type DP or DDP
    return model.module if is_parallel(model) else model


def initialize_weights(model):
    """
    åœ¨yolo.pyçš„modelç±»ä¸­çš„initå‡½æ•°è¢«è°ƒç”¨
    ç”¨äºåˆå§‹åŒ–æ¨¡å‹æƒé‡
    :param model:
    :return:
    """
    for m in model.modules():
        t = type(m)
        if t is nn.Conv2d:  # å¦‚æœæ˜¯äºŒç»´å·ç§¯å°±è·³è¿‡ï¼Œæˆ–è€…ä½¿ç”¨ä½•å‡¯æ˜åˆå§‹åŒ–
            pass  # nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
        elif t is nn.BatchNorm2d:  # å¦‚æœæ˜¯bnå±‚ï¼Œå°±è®¾ç½®ç›¸å…³å‚æ•°:epså’Œmomentum
            m.eps = 1e-3
            m.momentum = 0.03
        elif t in [nn.Hardswish, nn.LeakyReLU, nn.ReLU, nn.ReLU6, nn.SiLU]:
            # å¦‚æœæ˜¯è¿™å‡ ç±»æ¿€æ´»å‡½æ•°ï¼Œinplaceæ’å€¼å°±èµ‹ä¸ºTrue
            # inplace = TrueæŒ‡è¿›è¡ŒåŸåœ°æ“ä½œï¼Œå¯¹äºä¸Šå±‚ç½‘ç»œä¼ é€’ä¸‹æ¥çš„tensorç›´æ¥è¿›è¡Œä¿®æ”¹ï¼Œä¸éœ€è¦å¦å¤–èµ‹å€¼å˜é‡
            # è¿™æ ·å¯ä»¥èŠ‚çœè¿ç®—å†…å­˜ï¼Œä¸ç”¨å¤šå‚¨å­˜å˜é‡
            m.inplace = True


# æ²¡ç”¨åˆ°
def find_modules(model, mclass=nn.Conv2d):
    # Finds layer indices matching module class 'mclass'
    return [i for i, m in enumerate(model.module_list) if isinstance(m, mclass)]


# ç”¨æ¥è®¡ç®—æ¨¡å‹çš„ç¨€ç–æˆéƒ½sparsityï¼Œè¿”å›æ¨¡å‹æ•´ä½“çš„ç¨€ç–æ€§ï¼Œä¼šè¢«pruneå‰ªæå‡½æ•°ä¸­è¢«è°ƒç”¨
def sparsity(model):
    # Return global model sparsity
    a, b = 0, 0
    for p in model.parameters():
        a += p.numel()
        b += (p == 0).sum()
    return b / a


# ç”¨äºæ¨¡å‹å‰ªæçš„ï¼Œä»£ç ä¸­å¹¶æ²¡æœ‰ç”¨åˆ°
def prune(model, amount=0.3):
    # Prune model to requested global sparsity
    """
    å¯ç”¨äºtest.pyå’Œdetect.pyä¸­è¿›è¡Œæ¨¡å‹å‰ªæ
    :param model:
    :param amount:
    :return:
    """
    import torch.nn.utils.prune as prune
    print('Pruning model... ', end='')
    for name, m in model.named_modules():
        if isinstance(m, nn.Conv2d):
            prune.l1_unstructured(m, name='weight', amount=amount)  # prune
            prune.remove(m, 'weight')  # make permanent
    print(' %.3g global sparsity' % sparsity(model))


def fuse_conv_and_bn(conv, bn):
    """
    åœ¨yolo.pyä¸­Modelç±»çš„fuseå‡½æ•°ä¸­è°ƒç”¨
    èåˆå·ç§¯å±‚å’ŒBNå±‚ï¼ˆæµ‹è¯•æ¨ç†ä½¿ç”¨ï¼‰
    æ–¹æ³•ï¼šå·ç§¯å±‚è¿˜æ˜¯æ­£å¸¸å®šä¹‰ï¼Œä½†æ˜¯å·ç§¯å±‚çš„å‚æ•°w,bè¦æ”¹å˜ï¼Œé€šè¿‡åªæ”¹å˜å·ç§¯å‚æ•°ï¼Œè¾¾åˆ°conv+bnçš„æ•ˆæœ
    w = w_bn * w_conv b = w_bn * b_conv + b_bn(å¯ä»¥è¯æ˜)
    :param conv:torchæ”¯æŒçš„å·ç§¯å±‚
    :param bn:torchæ”¯æŒçš„bnå±‚
    :return:
    """
    # Fuse Conv2d() and BatchNorm2d() layers https://tehnokv.com/posts/fusing-batchnorm-and-conv/
    fusedconv = nn.Conv2d(conv.in_channels,
                          conv.out_channels,
                          kernel_size=conv.kernel_size,
                          stride=conv.stride,
                          padding=conv.padding,
                          groups=conv.groups,
                          bias=True).requires_grad_(False).to(conv.weight.device)

    # w_convï¼šå·ç§¯å±‚çš„wå‚æ•°
    # Prepare filters
    w_conv = conv.weight.clone().view(conv.out_channels, -1)
    w_bn = torch.diag(bn.weight.div(torch.sqrt(bn.eps + bn.running_var)))
    fusedconv.weight.copy_(torch.mm(w_bn, w_conv).view(fusedconv.weight.shape))

    # Prepare spatial bias
    b_conv = torch.zeros(conv.weight.size(0), device=conv.weight.device) if conv.bias is None else conv.bias
    b_bn = bn.bias - bn.weight.mul(bn.running_mean).div(torch.sqrt(bn.running_var + bn.eps))
    fusedconv.bias.copy_(torch.mm(w_bn, b_conv.reshape(-1, 1)).reshape(-1) + b_bn)

    return fusedconv


def model_info(model, verbose=False, img_size=640):
    """
    ç”¨äºyolo.pyæ–‡ä»¶çš„modelç±»çš„ynfoå‡½æ•°
    è¾“å‡ºæ¨¡å‹çš„æ‰€æœ‰ä¿¡æ¯ï¼ŒåŒ…æ‹¬ï¼šæ‰€æœ‰å±‚æ•°é‡ï¼Œæ¨¡å‹æ€»å‚æ•°é‡ï¼Œéœ€è¦æ±‚æ¢¯åº¦çš„æ€»å‚æ•°é‡ï¼Œimg_sizeå¤§å°çš„modelçš„æµ®ç‚¹è®¡ç®—é‡GFLOPs
    :param model:æ¨¡å‹
    :param verbose:æ˜¯å¦è¾“å‡ºæ¯ä¸€å±‚çš„å‚æ•°parametersçš„ç›¸å…³ä¿¡æ¯
    :param img_size: int or list img_size=640 or img_size=[640,320]
    :return:
    """
    # n_p:æ¨¡å‹modelçš„æ€»å‚æ•° number parameters
    # Model information. img_size may be int or list, i.e. img_size=640 or img_size=[640, 320]
    n_p = sum(x.numel() for x in model.parameters())  # number parameters
    # n_g:æ¨¡å‹modelçš„å‚æ•°ä¸­éœ€è¦æ±‚æ¢¯åº¦çš„å‚æ•°é‡
    n_g = sum(x.numel() for x in model.parameters() if x.requires_grad)  # number gradients
    if verbose:
        # è¡¨å¤´ï¼šâ€˜layerâ€™,'name','gradient'   , 'parameters' ,'shape'        ,'mu'        ,'sigma'
        #      'ç¬¬å‡ å±‚'ï¼Œ'å±‚å','æ˜¯å¦éœ€è¦æ±‚æ¢¯åº¦'ï¼Œ'å½“å‰å±‚å‚æ•°æ•°é‡'ï¼Œâ€˜å½“å‰å±‚å‚æ•°shapeâ€™â€˜å½“å‰å‚æ•°å‡å€¼â€™,'å½“å‰å±‚å‚æ•°æ–¹å·®'
        print(f"{'layer':>5} {'name':>40} {'gradient':>9} {'parameters':>12} {'shape':>20} {'mu':>10} {'sigma':>10}")
        # æŒ‰è¡¨å¤´è¾“å‡ºæ¯ä¸€å±‚çš„å‚æ•°parametersçš„ç›¸å…³ä¿¡æ¯
        for i, (name, p) in enumerate(model.named_parameters()):
            name = name.replace('module_list.', '')
            print('%5g %40s %9s %12g %20s %10.3g %10.3g' %
                  (i, name, p.requires_grad, p.numel(), list(p.shape), p.mean(), p.std()))

    try:  # FLOPs
        from thop import profile  # å¯¼å…¥è®¡ç®—æµ®ç‚¹è®¡ç®—é‡FLOPsçš„å·¥å…·åŒ…
        # strideï¼Œæ¨¡å‹çš„æœ€å¤§ä¸‹é‡‡æ ·ç‡ï¼Œæœ‰[8,16,32]ï¼Œæ‰€ä»¥stride=32
        stride = max(int(model.stride.max()), 32) if hasattr(model, 'stride') else 32
        # æ¨¡æ‹Ÿä¸€å¼ è¾“å…¥å›¾ç‰‡ï¼Œshape=(1,3,32,32)ï¼Œå…¨æ˜¯0
        img = torch.zeros((1, model.yaml.get('ch', 3), stride, stride), device=next(model.parameters()).device)  # input
        # è°ƒç”¨profileè®¡ç®—è¾“å…¥å›¾ç‰‡img=(1,3,32,32)æ—¶ï¼Œä½†é’±æ¨¡å‹çš„æµ®ç‚¹è®¡ç®—é‡GFLOPs
        # profileæ±‚å‡ºæ¥çš„æµ®ç‚¹è®¡ç®—é‡æ—¶FLOPs/1E9 => GFLOPs, *2æ˜¯å› ä¸ºprofileå‡½æ•°é»˜è®¤æ±‚çš„å°±æ˜¯æ¨¡å‹ä¸ºfloat64æ—¶çš„æµ®ç‚¹è®¡ç®—é‡
        # è€Œæˆ‘ä»¬ä¼ å…¥çš„æ¨¡å‹ä¸€èˆ¬éƒ½æ˜¯float32ï¼Œæ‰€ä»¥ä¹˜ä»¥2
        flops = profile(deepcopy(model), inputs=(img,), verbose=False)[0] / 1E9 * 2  # stride GFLOPs
        # expand img_size -> [img_size,img_size]=[640,640]
        img_size = img_size if isinstance(img_size, list) else [img_size, img_size]  # expand if int/float
        # æ ¹æ®img=(1,3,32,32)çš„æµ®ç‚¹è®¡ç®—é‡flopsæ¨ç®—å‡º640x640çš„å›¾ç‰‡çš„æµ®ç‚¹è®¡ç®—é‡
        fs = ', %.1f GFLOPs' % (flops * img_size[0] / stride * img_size[1] / stride)  # 640x640 GFLOPs
    except (ImportError, Exception):
        fs = ''

    name = Path(model.yaml_file).stem.replace('yolov5', 'YOLOv5') if hasattr(model, 'yaml_file') else 'Model'
    LOGGER.info(f"{name} summary: {len(list(model.modules()))} layers, {n_p} parameters, {n_g} gradients{fs}")


def scale_img(img, ratio=1.0, same_shape=False, gs=32):  # img(16,3,256,416)
    """
    ç”¨äºyolo.pyæ–‡ä»¶ä¸­modelç±»çš„forward_augmentå‡½æ•°ä¸­
    å®ç°å¯¹å›¾ç‰‡çš„ç¼©æ”¾æ“ä½œ
    :param img:åŸå›¾
    :param ratio:ç¼©æ”¾æ¯”ä¾‹ é»˜è®¤=1.0åŸå›¾
    :param same_shape:ç¼©æ”¾ä¹‹åå°ºå¯¸æ˜¯å¦è¦æ±‚çš„å¤§å°ï¼ˆå¿…é¡»æ˜¯gs=32çš„å€æ•°ï¼‰
    :param gs:æœ€å¤§çš„ä¸‹é‡‡æ ·ç‡ 32ï¼Œæ‰€ä»¥ç¼©æ”¾åçš„å›¾ç‰‡çš„shapeå¿…é¡»æ˜¯gs=32çš„å€æ•°
    :return:
    """
    # Scales img(bs,3,y,x) by ratio constrained to gs-multiple
    if ratio == 1.0:  # å¦‚æœç¼©æ”¾æ¯”ä¾‹ä¸º1.0,ç›´æ¥è¿”å›åŸå›¾
        return img
    else:
        # h,wï¼šåŸå›¾çš„é«˜å’Œå®½
        h, w = img.shape[2:]
        # s:ç¼©æ”¾åå›¾ç‰‡çš„æ–°å°ºå¯¸
        s = (int(h * ratio), int(w * ratio))  # new size
        # ç›´æ¥ä½¿ç”¨torchè‡ªå¸¦çš„F.interpolate(ä¸Šé‡‡æ ·ä¸‹é‡‡æ ·å‡½æ•°)æ’å€¼å‡½æ•°è¿›è¡Œresize
        # F.interpolate:å¯ä»¥ç»™å®šsizeæˆ–è€…scale_factoræ¥è¿›è¡Œä¸Šä¸‹é‡‡æ ·
        #                mode = 'bilinear':åŒçº¿æ€§æ’å€¼ nearest:æœ€è¿‘é‚»
        #                align_corner:æ˜¯å¦å¯¹é½inputå’Œoutputçš„è§’ç‚¹åƒç´ 
        img = F.interpolate(img, size=s, mode='bilinear', align_corners=False)  # resize
        if not same_shape:  # pad/crop img
            # ç¼©æ”¾ä¹‹åè¦æ˜¯å°ºå¯¸å’Œè¦æ±‚çš„å¤§å°ï¼ˆå¿…é¡»æ˜¯gs=32çš„å€æ•°ï¼‰ä¸åŒï¼Œå†å¯¹å…¶ä¸ç›¸äº¤çš„éƒ¨åˆ†è¿›è¡Œpad
            # è€Œpadçš„å€¼å°±æ˜¯imagenetçš„mean
            # math.ceil():å‘ä¸Šå–æ•´ï¼Œè¿™é‡Œé™¤ä»¥gså‘ä¸Šå–æ•´å†ä¹˜ä»¥gsæ˜¯ä¸ºäº†ä¿è¯hã€wéƒ½æ˜¯gsçš„å€æ•°
            h, w = (math.ceil(x * ratio / gs) * gs for x in (h, w))
        return F.pad(img, [0, w - s[1], 0, h - s[0]], value=0.447)  # value = imagenet mean


def copy_attr(a, b, include=(), exclude=()):
    """
    åœ¨modelEMAå‡½æ•°å’Œyolo.pyä¸­modelç±»çš„autoshapeå‡½æ•°ä¸­è°ƒç”¨
    å¤åˆ¶bçš„å±æ€§ï¼ˆè¿™ä¸ªå±æ€§å¿…é¡»åœ¨includeä¸­è€Œä¸åœ¨excludeä¸­ï¼‰
    :param a:å¯¹è±¡aï¼ˆå¾…èµ‹å€¼ï¼‰
    :param b:å¯¹è±¡bï¼ˆèµ‹å€¼ï¼‰
    :param include:å¯ä»¥èµ‹å€¼çš„å±æ€§
    :param exclude:ä¸èƒ½èµ‹å€¼çš„å±æ€§
    :return:
    """
    # Copy attributes from b to a, options to only include [...] and to exclude [...]
    for k, v in b.__dict__.items():
        if (len(include) and k not in include) or k.startswith('_') or k in exclude:
            continue
        else:
            # å°†å¯¹è±¡bçš„å±æ€§kèµ‹å€¼ç»™a
            setattr(a, k, v)


class EarlyStopping:
    # YOLOv5 simple early stopper
    def __init__(self, patience=30):
        self.best_fitness = 0.0  # i.e. mAP
        self.best_epoch = 0
        self.patience = patience or float('inf')  # epochs to wait after fitness stops improving to stop
        self.possible_stop = False  # possible stop may occur next epoch

    def __call__(self, epoch, fitness):
        if fitness >= self.best_fitness:  # >= 0 to allow for early zero-fitness stage of training
            self.best_epoch = epoch
            self.best_fitness = fitness
        delta = epoch - self.best_epoch  # epochs without improvement
        self.possible_stop = delta >= (self.patience - 1)  # possible stop may occur next epoch
        stop = delta >= self.patience  # stop training if patience exceeded
        if stop:
            LOGGER.info(f'Stopping training early as no improvement observed in last {self.patience} epochs. '
                        f'Best results observed at epoch {self.best_epoch}, best model saved as best.pt.\n'
                        f'To update EarlyStopping(patience={self.patience}) pass a new patience value, '
                        f'i.e. `python train.py --patience 300` or use `--patience 0` to disable EarlyStopping.')
        return stop


# å¸¸è§çš„æé«˜æ¨¡å‹çš„é²æ£’æ€§çš„å¢å¼ºtrockï¼Œè¢«å¹¿æ³›çš„ä½¿ç”¨ï¼Œå…¨åï¼šæ¨¡å‹çš„æŒ‡æ•°åŠ æƒå¹³å‡æ–¹æ³•
class ModelEMA:
    """ Updated Exponential Moving Average (EMA) from https://github.com/rwightman/pytorch-image-models
    Keeps a moving average of everything in the model state_dict (parameters and buffers)
    For EMA details see https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage
    """

    def __init__(self, model, decay=0.9999, tau=2000, updates=0):
        # Create EMA
        """
        train.py
        model:
        decay:è¡°å‡å‡½æ•°å‚æ•°ï¼Œé»˜è®¤0.9999ï¼Œè€ƒè™‘è¿‡å»10000æ¬¡çš„çœŸå®å€¼
        :param model:
        :param decay:
        :param tau:
        :param updates:
        """
        # åˆ›å»ºemaæ¨¡å‹
        self.ema = deepcopy(de_parallel(model)).eval()  # FP32 EMA
        # if next(model.parameters()).device.type != 'cpu':
        #     self.ema.half()  # FP16 EMA
        self.updates = updates  # number of EMA updates
        self.decay = lambda x: decay * (1 - math.exp(-x / tau))  # decay exponential ramp (to help early epochs)
        # æ‰€æœ‰å‚æ•°å–æ¶ˆè®¾ç½®æ¢¯åº¦
        for p in self.ema.parameters():
            p.requires_grad_(False)

    def update(self, model):
        # Update EMA parameters
        with torch.no_grad():
            self.updates += 1
            d = self.decay(self.updates)
            # msd:æ¨¡å‹é…ç½®çš„å­—å…¸ï¼Œmodel state_dict,msdä¸­çš„æ•°æ®ä¿æŒä¸å˜ï¼Œç”¨äºè®­ç»ƒ
            msd = de_parallel(model).state_dict()  # model state_dict
            # éå†æ¨¡å‹é…ç½®å­—å…¸
            for k, v in self.ema.state_dict().items():
                # è¿™é‡Œå¾—åˆ°çš„v:é¢„æµ‹å€¼
                if v.dtype.is_floating_point:
                    v *= d
                    v += (1 - d) * msd[k].detach()

    def update_attr(self, model, include=(), exclude=('process_group', 'reducer')):
        # Update EMA attributesï¼Œä»modelä¸­å¤åˆ¶ç›¸å…³å±æ€§å€¼åˆ°self.emaä¸­
        copy_attr(self.ema, model, include, exclude)
